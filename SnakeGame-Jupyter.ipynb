{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c4d8cc-fbcc-4a0c-8b32-a40a0da65225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pygame numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fe770-34e7-4dfb-b00d-6106e7332b3b",
   "metadata": {},
   "source": [
    "### Importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e53358-d52e-4b0a-b6aa-ba17b50afeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame  # Spielentwicklung\n",
    "import random  \n",
    "import numpy as np  \n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "import os  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import Enum  # Aufzählungen\n",
    "from collections import namedtuple  # Benannte Tupel\n",
    "from collections import deque  # Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63109a8-28be-4ef3-945b-429e015fa654",
   "metadata": {},
   "source": [
    "### Hauptkomponenten des Spiels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f6a0d2-10f5-4326-ba9f-d18e8ab6a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "font = pygame.font.SysFont('Arial', 30)\n",
    "\n",
    "# Farben\n",
    "WHITE = (255, 255, 255)\n",
    "BEIGE = (245, 245, 220)  # Hintergrund\n",
    "LIGHT_GREEN = (144, 238, 144)  # für den Körper\n",
    "DARK_GREEN = (0, 100, 0)  # für den Kopf\n",
    "RED = (200, 0, 0)  # für das Futter\n",
    "BLACK = (0, 0, 0)  # für den Score\n",
    "GOLD = (255, 215, 0)  # Bonus\n",
    "DARK_GRAY = (169, 169, 169)  # für den Rahmen und den Hintergrund\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 15\n",
    "MAX_FRAME_ITERATION = 100 * BLOCK_SIZE  # Maximale Schritte pro Schlange\n",
    "\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "\n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "class SnakeGameAI:\n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w  # Setzt die Breite des Fensters\n",
    "        self.h = h  # Setzt die Höhe des Fensters\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake Game AI')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Setzt den Zustand des Spiels zurück.\"\"\"\n",
    "        self.direction = Direction.RIGHT\n",
    "        self.head = Point(self.w / 2, self.h / 2)  # Setzt die Position des Schlangenkopfes\n",
    "        self.snake = [self.head,\n",
    "                      Point(self.head.x - BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x - (2 * BLOCK_SIZE), self.head.y)]\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "        self.level = 1\n",
    "        self.speed_increase = 2  # Geschwindigkeitserhöhung pro Level\n",
    "        self.speed_by_level = {\n",
    "            1: 10,\n",
    "            2: 20,\n",
    "            3: 30,\n",
    "            4: 40,\n",
    "            5: 50,\n",
    "            6: 60,\n",
    "            7: 70,\n",
    "            8: 80,\n",
    "            9: 90,\n",
    "            10: 100, \n",
    "        }\n",
    "        self.reset_speed()  # Setzt die Geschwindigkeit entsprechend dem Level\n",
    "\n",
    "    def reset_speed(self):\n",
    "        \"\"\"Setzt die Geschwindigkeit des Spiels auf den Wert des aktuellen Levels.\"\"\"\n",
    "        global SPEED\n",
    "        SPEED = self.speed_by_level.get(self.level, 15)  # Setzt die Geschwindigkeit basierend auf dem Level\n",
    "\n",
    "    def _place_food(self):\n",
    "        \"\"\"Platziert das Futter an einer zufälligen Position.\"\"\"\n",
    "        x = random.randint(0, (self.w - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE  # Berechnet die x-Position des Futters.\n",
    "        y = random.randint(0, (self.h - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:  # Überprüft, ob das Futter zufällig auf einem Schlangensegment liegt und platziert es dann neu.\n",
    "            self._place_food()\n",
    "\n",
    "    def play_step(self, action):\n",
    "        \"\"\"Ein Schritt im Spiel.\"\"\"\n",
    "        self.frame_iteration += 1\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "\n",
    "        self._move(action)\n",
    "        self.snake.insert(0, self.head)\n",
    "\n",
    "        if self.is_collision() or self.frame_iteration > MAX_FRAME_ITERATION:\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "\n",
    "            # Steigerung der Geschwindigkeit jedes Mal, wenn die Schlange wächst\n",
    "            if self.score % 10 == 0:\n",
    "                #self.level += 1\n",
    "                self.reset_speed()  # Geschwindigkeit an das neue Level anpassen\n",
    "                print(f\"Level up! Level: {self.level}, Speed: {SPEED}\")\n",
    "\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        return reward, game_over, self.score\n",
    "\n",
    "    def is_collision(self, pt=None):\n",
    "        \"\"\"Überprüft Kollisionen mit den Wänden oder der Schlange selbst.\"\"\"\n",
    "        if pt is None:\n",
    "            pt = self.head\n",
    "        if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
    "            return True\n",
    "        if pt in self.snake[1:]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _update_ui(self):\n",
    "        \"\"\"Aktualisiert das UI, einschließlich Hintergrund, Schlange, Futter und Punktzahl.\"\"\"\n",
    "        self.display.fill(BEIGE)  # Beige Hintergrund\n",
    "        self._draw_border()\n",
    "        self._draw_snake()\n",
    "        self._draw_food()\n",
    "        self._draw_score()\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def _draw_border(self):\n",
    "        \"\"\"Zeichnet einen Rahmen um das Spielfeld.\"\"\"\n",
    "        pygame.draw.rect(self.display, DARK_GRAY, pygame.Rect(0, 0, self.w, self.h), 5)\n",
    "\n",
    "    def _draw_snake(self):\n",
    "        \"\"\"Zeichnet die Schlange auf dem Bildschirm.\"\"\"\n",
    "        for i, pt in enumerate(self.snake):\n",
    "            if i == 0:  # Kopf der Schlange\n",
    "                pygame.draw.circle(self.display, DARK_GREEN, (pt.x + BLOCK_SIZE // 2, pt.y + BLOCK_SIZE // 2), BLOCK_SIZE // 2)\n",
    "            else:  # Körper der Schlange\n",
    "                pygame.draw.circle(self.display, LIGHT_GREEN, (pt.x + BLOCK_SIZE // 2, pt.y + BLOCK_SIZE // 2), BLOCK_SIZE // 2)\n",
    "\n",
    "    def _draw_food(self):\n",
    "        \"\"\"Zeichnet das Futter.\"\"\"\n",
    "        pygame.draw.circle(self.display, RED, (self.food.x + BLOCK_SIZE // 2, self.food.y + BLOCK_SIZE // 2), BLOCK_SIZE // 2)\n",
    "\n",
    "    def _draw_score(self):\n",
    "        \"\"\"Zeigt den aktuellen Punktestand an.\"\"\"\n",
    "        score_text = font.render(f\"Score: {self.score}  Level: {self.level}\", True, BLACK)\n",
    "        self.display.blit(score_text, [10, 10])\n",
    "\n",
    "    def _move(self, action):\n",
    "        \"\"\"Bewegt die Schlange basierend auf der Eingabeaktion.\"\"\"\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_dir = clock_wise[idx]  # Keine Änderung\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_idx = (idx + 1) % 4\n",
    "            new_dir = clock_wise[next_idx]  # Rechtsdrehung\n",
    "        else:\n",
    "            next_idx = (idx - 1) % 4\n",
    "            new_dir = clock_wise[next_idx]  # Linksdrehung\n",
    "\n",
    "        self.direction = new_dir\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "\n",
    "        self.head = Point(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ed8d3-8951-40ab-a5d9-290e48c77624",
   "metadata": {},
   "source": [
    "### Grundlegenden Bausteine für ein Deep- Q-Learning-Modell mit einem neuronalen Netzwerk\n",
    "#### ( Regeln, Umgebung, neuronales Netzwerk, Trainingslogik )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddacb920-8f66-488d-9a79-84db3d19d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanten\n",
    "MAX_MEMORY = 100_000  # Maximale Anzahl von gespeicherten Erfahrungen.(auf vergangene Erfahrungen zurückgreifen)\n",
    "BATCH_SIZE = 1000  # Anzahl der Erfahrungen, die in einem Schritt zum Training verwendet werden.\n",
    "LR = 0.001  # Lernrate: Bestimmt, wie stark das Modell bei jedem Schritt angepasst wird.\n",
    "\n",
    "\n",
    "# Definition der Klasse für das Q-Learning-Modell\n",
    "class Linear_QNet(nn.Module):  #Ein neuronales Netz der geschätzten Q-Werte.\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)  # Erste lineare Schicht\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)  # Zweite lineare Schicht\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  # Aktivierung der ersten Schicht mit ReLU (Nichtlineare Bz lernen:komplexe Beziehungen)\n",
    "        x = self.linear2(x)          # Ausgabe durch die zweite Schicht\n",
    "        return x                     \n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'  # Ordner für das Speichern der Modelle\n",
    "        if not os.path.exists(model_folder_path):  \n",
    "            os.makedirs(model_folder_path)        \n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name) \n",
    "        torch.save(self.state_dict(), file_name)               # Speichern der Modellparameter\n",
    "\n",
    "# Definition der Trainingslogik für Q-Learning (Bestandteil RL)\n",
    "class QTrainer: \n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr  \n",
    "        self.gamma = gamma                     \n",
    "        self.model = model                     \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)  # Adam-Optimierer (verbessert klassischen Gradientenabstiegsalgorithmus)\n",
    "        self.criterion = nn.MSELoss()          # Verlustfunktion: Mean Squared Error(Vorhersagewert optimieren)\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Konvertieren der Eingaben in Tensoren (NN Erforderlich)\n",
    "        state = torch.tensor(state, dtype=torch.float32)       # Zustand\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)  \n",
    "        action = torch.tensor(action, dtype=torch.long)        # Aktion\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)     # Belohnung\n",
    "\n",
    "        \n",
    "        if len(state.shape) == 1:              # Falls die Eingabe eindimensional ist\n",
    "            state = torch.unsqueeze(state, 0)  # Hinzufügen einer Batch-Dimension\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )                    # Umwandeln von done in ein Tupel\n",
    "\n",
    "        # Vorhersage der aktuellen Q-Werte\n",
    "        pred = self.model(state)               \n",
    "\n",
    "        # Berechnung der Ziel-Q-Werte\n",
    "        target = pred.clone()                  # Kopieren der vorhergesagten Q-Werte\n",
    "        for idx in range(len(done)):           # Iteration über alle Einträge im Batch\n",
    "            Q_new = reward[idx]                # Initialer Q-Wert ist die Belohnung\n",
    "            if not done[idx]:                  # Wenn nicht abgeschlossen \n",
    "                with torch.no_grad():          # Keine Gradientenberechnung\n",
    "                    Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))  # Aktualisieren\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new  # Ziel-Q-Wert setzen\n",
    "\n",
    "        # Berechnung des Verlusts und Backpropagation\n",
    "        self.optimizer.zero_grad()             # Zurücksetzen der Gradienten\n",
    "        loss = self.criterion(target, pred)    # Verlust berechnen\n",
    "        loss.backward()                        # Backpropagation durchführen\n",
    "        self.optimizer.step()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348b4c30-da80-4530-9516-89980b1cbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanten\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001\n",
    "\n",
    "# Erweiterte Q-Network Architektur\n",
    "class Deep_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "        \n",
    "        pred = self.model(state)\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            Q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                with torch.no_grad():\n",
    "                    Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fe390-af95-4c6a-aebf-b83ba215dfdf",
   "metadata": {},
   "source": [
    "![Bildbeschreibung](https://lh4.googleusercontent.com/proxy/KQMB740gkpqWidYF22kaJbhvRU8t3PX4tDA1FIEwPyEOCqGtzoh4fjuQohi20huVbpeL0ldkhIozcpc5rlU7TZhBHCxTyAKOaFdp3OmndZ-5Uo3KRNoOag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa56062-844a-41a1-8b98-6b34daadf591",
   "metadata": {},
   "source": [
    "### Agenten-basiertes Deep Q-Learning Modell\n",
    "#### (Beobachtet, trifft Entscheidungen, lernt und agiert in der Umgebung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98060480-22d8-4eca-8538-4ba78fbf23e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 17:42:28.401 Python[94569:9151897] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-12 17:42:28.401 Python[94569:9151897] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 120\u001b[0m\n\u001b[1;32m    116\u001b[0m             plot(plot_scores, plot_mean_scores)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 95\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m state_old \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)  \u001b[38;5;66;03m# Holen des aktuellen Zustands\u001b[39;00m\n\u001b[1;32m     94\u001b[0m final_move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state_old)  \u001b[38;5;66;03m# Auswahl der Aktion durch den Agenten\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Spielt die Aktion aus und erhält Belohnung\u001b[39;00m\n\u001b[1;32m     96\u001b[0m state_new \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)  \u001b[38;5;66;03m# Neuer Zustand nach der Aktion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_short_memory(state_old, final_move, reward, state_new, done)  \u001b[38;5;66;03m# Schnelles Training für aktuellen Schritt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 109\u001b[0m, in \u001b[0;36mSnakeGameAI.play_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ui\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(SPEED)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward, game_over, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mSnakeGameAI._update_ui\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_ui\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Aktualisiert das UI, einschließlich Hintergrund, Schlange, Futter und Punktzahl.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBEIGE\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Beige Hintergrund\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_draw_border()\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_draw_snake()\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "class Agent: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0  # zufälligkeit\n",
    "        self.gamma = 0.9  \n",
    "        self.memory = deque(maxlen=MAX_MEMORY)  # Replay Memory für Erfahrungen. .(auf vergangene Erfahrungen zurückgreifen)\n",
    "        self.model = Linear_QNet(11, 256, 3) #11 Eingabewerte, 256 Neuronen im versteckten Layer und 3 Ausgänge\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "    # Berechnet Zustand des Spielers\n",
    "    def get_state(self, game):\n",
    "        head = game.snake[0]\n",
    "        point_l = Point(head.x - 20, head.y)\n",
    "        point_r = Point(head.x + 20, head.y)\n",
    "        point_u = Point(head.x, head.y - 20)\n",
    "        point_d = Point(head.x, head.y + 20)\n",
    "\n",
    "        dir_l = game.direction == Direction.LEFT\n",
    "        dir_r = game.direction == Direction.RIGHT\n",
    "        dir_u = game.direction == Direction.UP\n",
    "        dir_d = game.direction == Direction.DOWN\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(point_r)) or \n",
    "            (dir_l and game.is_collision(point_l)) or \n",
    "            (dir_u and game.is_collision(point_u)) or \n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(point_r)) or \n",
    "            (dir_d and game.is_collision(point_l)) or \n",
    "            (dir_l and game.is_collision(point_u)) or \n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(point_r)) or \n",
    "            (dir_u and game.is_collision(point_l)) or \n",
    "            (dir_r and game.is_collision(point_u)) or \n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "\n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "\n",
    "            # Food location \n",
    "            game.food.x < game.head.x,  # food left\n",
    "            game.food.x > game.head.x,  # food right\n",
    "            game.food.y < game.head.y,  # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "        ]\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # Speichert die Erfahrung\n",
    "\n",
    "    def train_long_memory(self): # Nutzt einen Mini-Batch von gespeicherten Erfahrungen\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) \n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done): # Trainiert das Modell direkt nach einer Aktion\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 80 - self.n_games  # Exploration (neue Aktionen erkunden) nimmt mit der Anzahl der Spiele ab\n",
    "\n",
    "        if random.randint(0, 200) < self.epsilon:  # Wenn Exploration nötig ist\n",
    "            move = random.randint(0, 2)  # Zufällige Aktion (Exploration)\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)  # Konvertiert Zustand in Tensor\n",
    "            prediction = self.model(state0)  # Lässt Modell eine Vorhersage machen\n",
    "            move = torch.argmax(prediction).item()  # Wählt die Aktion mit dem höchsten Q-Wert\n",
    "        final_move = [0, 0, 0]  # Initialisiert die Bewegungsaktion\n",
    "        final_move[move] = 1  # Setzt die ausgewählte Aktion auf 1\n",
    "        return final_move  # Gibt die kodierte Aktion zurück\n",
    "\n",
    "\n",
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "    while True:\n",
    "        state_old = agent.get_state(game)  # Holen des aktuellen Zustands\n",
    "        final_move = agent.get_action(state_old)  # Auswahl der Aktion durch den Agenten\n",
    "        reward, done, score = game.play_step(final_move)  # Spielt die Aktion aus und erhält Belohnung\n",
    "        state_new = agent.get_state(game)  # Neuer Zustand nach der Aktion\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)  # Schnelles Training für aktuellen Schritt\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)  # Speichert den Schritt in Replay Memory\n",
    "        \n",
    "        if done:\n",
    "            game.reset()  # Startet das Spiel neu\n",
    "            agent.n_games += 1  # Erhöht die Spielanzahl\n",
    "            agent.train_long_memory()  # Trainiert das Modell mit gespeicherten Erinnerungen\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game:', agent.n_games, 'Score:', score, 'Record:', record)  # Ausgabe des aktuellen Spiels, Punktestand und Rekord\n",
    "            plot_scores.append(score) \n",
    "            total_score += score  # Addiere den aktuellen Punktestand zum Gesamtscore\n",
    "            mean_score = total_score / agent.n_games  \n",
    "            plot_mean_scores.append(mean_score)  \n",
    "\n",
    "            \n",
    "            plot(plot_scores, plot_mean_scores)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd3d67-e59c-49e7-91c1-1513122ce5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68fb0d-8e63-4805-b24d-ebef2d32931c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b4e6e-ad75-498d-b8fd-b2d17abc2876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
